Kafka为何需要High Available

为何需要Replication
在Kafka在0.8以前的版本中，是没有Replication的，一旦某一个Broker宕机，则其上所有的Partition数据都不可被消费，这与Kafka数据持久性及
Delivery Guarantee的设计目标相悖。同时Producer都不能再将数据存于这些Partition中。
如果Producer使用同步模式则Producer会在尝试重新发送message.send.max.retries（默认值为3）次后抛出Exception，用户可以选择停止发送
后续数据也可选择继续选择发送。而前者会造成数据的阻塞，后者会造成本应发往该Broker的数据的丢失。
如果Producer使用异步模式，则Producer会尝试重新发送message.send.max.retries（默认值为3）次后记录该异常并继续发送后续数据，这会造成
数据丢失并且用户只能通过日志发现该问题。同时，Kafka的Producer并未对异步模式提供callback接口。
由此可见，在没有Replication的情况下，一旦某机器宕机或者某个Broker停止工作则会造成整个系统的可用性降低。随着集群规模的增加，整个集群中
出现该类异常的几率大大增加，因此对于生产系统而言Replication机制的引入非常重要。

为何需要Leader Election
注意：本文所述Leader Election主要指Replica之间的Leader Election。
引入Replication之后，同一个Partition可能会有多个Replica，而这时需要在这些Replication之间选出一个Leader，Producer和Consumer只与
这个Leader交互，其它Replica作为Follower从Leader中复制数据。
因为需要保证同一个Partition的多个Replica之间的数据一致性（其中一个宕机后其它Replica必须要能继续服务并且即不能造成数据重复也不能造成数
据丢失）。如果没有一个Leader，所有Replica都可同时读/写数据，那就需要保证多个Replica之间互相（N×N条通路）同步数据，数据的一致性和有序
性非常难保证，大大增加了Replication实现的复杂性，同时也增加了出现异常的几率。而引入Leader后，只有Leader负责数据读写，Follower只向
Leader顺序Fetch数据（N条通路），系统更加简单且高效。

Kafka HA设计解析
如何将所有Replica均匀分布到整个集群
为了更好的做负载均衡，Kafka尽量将所有的Partition均匀分配到整个集群上。一个典型的部署方式是一个Topic的Partition数量大于Broker的数
量。同时为了提高Kafka的容错能力，也需要将同一个Partition的Replica尽量分散到不同的机器。实际上，如果所有的Replica都在同一个Broker
上，那一旦该Broker宕机，该Partition的所有Replica都无法工作，也就达不到HA的效果。同时，如果某个Broker宕机了，需要保证它上面的负载可
以被均匀的分配到其它幸存的所有Broker上。

Kafka的Data Replication需要解决如下问题：
怎样Propagate消息
在向Producer发送ACK前需要保证有多少个Replica已经收到该消息
怎样处理某个Replica不工作的情况
怎样处理Failed Replica恢复回来的情况

Propagate消息
Producer在发布消息到某个Partition时，先通过ZooKeeper找到该Partition的Leader，然后无论该Topic的Replication Factor为多少（也
即该Partition有多少个Replica），Producer只将该消息发送到该Partition的Leader。Leader会将该消息写入其本地Log。每个Follower都从
Leader pull数据。这种方式上，Follower存储的数据顺序与Leader保持一致。Follower在收到该消息并写入其Log后，向Leader发送ACK。一旦
Leader收到了ISR中的所有Replica的ACK，该消息就被认为已经commit了，Leader将增加HW并且向Producer发送ACK。
为了提高性能，每个Follower在接收到数据后就立马向Leader发送ACK，而非等到数据写入Log中。因此，对于已经commit的消息，Kafka只能保证它
被存于多个Replica的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被Consumer消费。但考虑到这种场景非
常少见，可以认为这种方式在性能和数据持久化上做了一个比较好的平衡。在将来的版本中，Kafka会考虑提供更高的持久性。
Consumer读消息也是从Leader读取，只有被commit过的消息（offset低于HW的消息）才会暴露给Consumer。

ACK前需要保证有多少个备份
和大部分分布式系统一样，Kafka处理失败需要明确定义一个Broker是否“活着”。对于Kafka而言，Kafka存活包含两个条件，一是它必须维护与
ZooKeeper的session（这个通过ZooKeeper的Heartbeat机制来实现）。二是Follower必须能够及时将Leader的消息复制过来，不能“落后太多”。
Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica）。如果一个Follower宕机，或者落后太多，Leader将把它从
ISR中移除。这里所描述的“落后太多”指Follower复制的消息落后于Leader后的条数超过预定值（该值可在$KAFKA_HOME/config/
server.properties中通过replica.lag.max.messages配置，其默认值是4000）或者Follower超过一定时间（该值可在
$KAFKA_HOME/config/server.properties中通过replica.lag.time.max.ms来配置，其默认值是10000）未向Leader发送fetch请求。

Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，完全同步复制要求所有能工作的Follower都复制完，这条消息才会被认为
commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。而异步复制方式下，Follower异步的从Leader复制数据，数据
只要被Leader写入log就被认为已经commit，这种情况下如果Follower都复制完都落后于Leader，而如果Leader突然宕机，则会丢失数据。而Kafka
的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），
极大减少了Follower与Leader的差距。

需要说明的是，Kafka只解决fail/recover，不处理“Byzantine”（“拜占庭”）问题。一条消息只有被ISR里的所有Follower都从Leader复制过去才
会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）
。而对于Producer而言，它可以选择是否等待消息commit，这可以通过request.required.acks来设置。这种机制确保了只要ISR有一个或以上的
Follower，一条被commit的消息就不会丢失。

如何处理所有Replica都不工作
上文提到，在ISR中至少有一个follower时，Kafka可以确保已经commit的数据不丢失，但如果某个Partition的所有Replica都宕机了，就无法保证数
据不丢失了。这种情况下有两种可行的方案：
	等待ISR中的任一个Replica“活”过来，并且选它作为Leader
	选择第一个“活”过来的Replica（不一定是ISR中的）作为Leader
这就需要在可用性和一致性当中作出一个简单的折衷。如果一定要等待ISR中的Replica“活”过来，那不可用的时间就可能会相对较长。而且如果ISR中的
所有Replica都无法“活”过来了，或者数据都丢失了，这个Partition将永远不可用。选择第一个“活”过来的Replica作为Leader，而这个Replica不
是ISR中的Replica，那即使它并不保证已经包含了所有已commit的消息，它也会成为Leader而作为consumer的数据源（前文有说明，所有读写都由
Leader完成）。Kafka0.8.*使用了第二种方式。根据Kafka的文档，在以后的版本中，Kafka支持用户通过配置选择这两种方式中的一种，从而根据不
同的使用场景选择高可用性还是强一致性。


下面测试使用kafka，主要介绍kafka的topic、partition、replication-factor的关系，以及如何通过zookeeper实现高可用的kafka集群。
创建一个topic：
./kafka-topics.sh --create --zookeeper 192.168.192.145:2181 --replication-factor 3 --partitions 5 --topic testp3
查看分区情况：
./kafka-topics.sh --describe --topic testp3 --zookeeper 192.168.192.145:2181
Topic:testp3    PartitionCount:5        ReplicationFactor:3     Configs:
        Topic: testp3   Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1
        Topic: testp3   Partition: 1    Leader: 1       Replicas: 1,2,0 Isr: 1,0
        Topic: testp3   Partition: 2    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1
        Topic: testp3   Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0,1
        Topic: testp3   Partition: 4    Leader: 1       Replicas: 1,0,2 Isr: 1,0
一个broker挂了，其他broker能顶替上来，比如挂掉broken2-147：
再次查看分区：
./kafka-topics.sh --describe --topic testp3 --zookeeper 192.168.192.145:2181
        Topic: testp3   Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1
        Topic: testp3   Partition: 1    Leader: 1       Replicas: 1,2,0 Isr: 1,0
		可以看到broker2节点杀死之后，leader做了切换，follower中做出了选择，从变化中可以看到isr这项表示的是该partition当前的leader
		和follower节点
        Topic: testp3   Partition: 2    Leader: 0       Replicas: 2,0,1 Isr: 0,1
        Topic: testp3   Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0,1
        Topic: testp3   Partition: 4    Leader: 1       Replicas: 1,0,2 Isr: 1,0
再杀掉broken1-146：
Topic:testp3    PartitionCount:5        ReplicationFactor:3     Configs:
        Topic: testp3   Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0
        Topic: testp3   Partition: 1    Leader: 0       Replicas: 1,2,0 Isr: 0
        Topic: testp3   Partition: 2    Leader: 0       Replicas: 2,0,1 Isr: 0
        Topic: testp3   Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0
        Topic: testp3   Partition: 4    Leader: 0       Replicas: 1,0,2 Isr: 0
再把所有杀死的broken重启：
Topic:testp3    PartitionCount:5        ReplicationFactor:3     Configs:
        Topic: testp3   Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1
        Topic: testp3   Partition: 1    Leader: 0       Replicas: 1,2,0 Isr: 0,1
        Topic: testp3   Partition: 2    Leader: 0       Replicas: 2,0,1 Isr: 0,1
        Topic: testp3   Partition: 3    Leader: 0       Replicas: 0,2,1 Isr: 0,1
        Topic: testp3   Partition: 4    Leader: 0       Replicas: 1,0,2 Isr: 0,1
也不会重新分配leader，可以用工具：./kafka-preferred-replica-election.sh --zookeeper 192.168.192.145:2181


还可以通过zookeeper查看：
ls /brokers/ids
[2, 1, 0] -----------通过这个命令行可以看到有3个brokers

get /brokers/ids/2
{"jmx_port":-1,"timestamp":"1492054322225","endpoints":["PLAINTEXT://192.168.192.147:9092"],"host":"192.168.192.147","version":3,"port":9092}
cZxid = 0xd000004ea
ctime = Thu Apr 13 11:44:20 CST 2017
mZxid = 0xd000004ea
mtime = Thu Apr 13 11:44:20 CST 2017
pZxid = 0xd000004ea
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x35b64e7c2560003--这是一个临时节点，说明如果broker被kill掉的话这个节点就会消失
dataLength = 141
numChildren = 0

接着查看partition的信息
ls /brokers/topics
[testp5, input_channel, testp4, output_channel, testp3, lighthouse.config.inform, testp2, netshoesProducts, testp, springcloudtest, __consumer_offsets, testp6]
ls /brokers/topics/testp5/partitions
[0, 1, 2, 3, 4]----------建立了5个partition
get /brokers/topics/testp5/partitions/3/state--------获取partition3的节点信息，表明这个是用broker0作为leader，1作为flower
{"controller_epoch":17,"leader":0,"version":1,"leader_epoch":1,"isr":[0,1]}
cZxid = 0xd000003b0
ctime = Thu Apr 13 11:24:05 CST 2017
mZxid = 0xd0000044d
mtime = Thu Apr 13 11:24:53 CST 2017
pZxid = 0xd000003b0
cversion = 0
dataVersion = 2
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 75
numChildren = 0
